{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2: Bigram Language Model and Generative Pretrained Transformer (GPT)\n",
        "\n",
        "\n",
        "The objective of this assignment is to train a simplified transformer model. The primary differences between the implementation:\n",
        "* tokenizer (we use a character level encoder simplicity and compute constraints)\n",
        "* size (we are using 1 consumer grade gpu hosted on colab and a small dataset. in practice, the models are much larger and are trained on much more data)\n",
        "* efficiency\n",
        "\n",
        "\n",
        "Most modern LLMs have multiple training stages, so we won't get a model that is capable of replying to you yet. However, this is the first step towards a model like ChatGPT and Llama.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fYCVVv_1A1kZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F097yaiu7dXQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "U021NSeZfS8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Bigram MLP for TinyShakespeare (35 points)\n",
        "\n",
        "1a) (1 point). Create a list `chars` that contains all unique characters in `text`\n",
        "\n",
        "1b) (2 points). Implement `encode(s: str) -> list[int]`\n",
        "\n",
        "1c) (2 points). Implement `decode(ids: list[int]) -> str`\n",
        "\n",
        "1d) (5 points). Create two tensors, `inputs_one_hot` and `outputs_one_hot`. Use one hot encoding. Make sure to get every consecutive pair of characters. For example, for the word 'hello', we should create the following input-output pairs\n",
        "```\n",
        "he\n",
        "el\n",
        "ll\n",
        "lo\n",
        "```\n",
        "\n",
        "1e) (10 points). Implement BigramOneHotMLP, a 2 layer MLP that predicts the next token. Specifically, implement the constructor, forward, and generate. The output dimension of the first layer should be 8. Use `torch.optim`. The activation function for the first layer should be `nn.LeakyReLU()`\n",
        "\n",
        "Note: Use the `torch.nn.function.cross_entropy` loss. Read the [docs](https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html) about how this loss function works. The logits are the output of a network WITHOUT an activation function applied to the last layer. There are activation functions are applied to every layer except the last.\n",
        "\n",
        "1f) (5 points). Train the BigramOneHotMLP for 1000 steps.\n",
        "\n",
        "1g) (5 points). Create two tensors, `input_ids` and `outputs_one_hot`. These `input_ids` will be used for the embedding layer.\n",
        "\n",
        "1h) (5 points). Implement and train BigramEmbeddingMLP, a 2 layer mlp that predicts the next token. Specifically, implement the constructor, forward, and generate functions. The output dimension of the first layer should be 8. Use `torch.optim`.\n",
        "\n",
        "\n",
        "\n",
        "Note: the output will look like gibberish\n"
      ],
      "metadata": {
        "id": "8qra06Ema_VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iradmn7bZtM",
        "outputId": "3d885306-4671-4fc3-de3a-25ee3f1ec464"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-14 02:57:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-10-14 02:57:13 (48.3 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the bigram model, let's use the first 1000 characters for the data\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "text = text[:1000]"
      ],
      "metadata": {
        "id": "pLoVi294G-T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1a) Create a list of unique characters\n",
        "chars = sorted(list(set(text)))\n",
        "\n",
        "# 1b) Implement encode function\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [chars.index(c) for c in s]\n",
        "\n",
        "# 1c) Implement decode function\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join([chars[i] for i in ids])\n",
        "\n",
        "# 1d) Create one-hot inputs and outputs\n",
        "def create_one_hot_inputs_and_outputs() -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    inputs, outputs = [], []\n",
        "    for i in range(len(text) - 1):\n",
        "        input_char = text[i]\n",
        "        output_char = text[i + 1]\n",
        "        inputs.append(F.one_hot(torch.tensor(chars.index(input_char)), num_classes=len(chars)))\n",
        "        outputs.append(F.one_hot(torch.tensor(chars.index(output_char)), num_classes=len(chars)))\n",
        "    return torch.stack(inputs), torch.stack(outputs)\n",
        "\n",
        "inputs_one_hot, outputs_one_hot = create_one_hot_inputs_and_outputs()\n",
        "\n",
        "# 1e) Implement BigramOneHotMLP\n",
        "class BigramOneHotMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(len(chars), 8)\n",
        "        self.fc2 = nn.Linear(8, len(chars))\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        out = start\n",
        "        for _ in range(max_new_tokens):\n",
        "            x = F.one_hot(torch.tensor(chars.index(out[-1])), num_classes=len(chars)).float()\n",
        "            logits = self(x)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_char = chars[torch.multinomial(probs, 1).item()]\n",
        "            out += next_char\n",
        "        return out\n",
        "\n",
        "bigram_one_hot_mlp = BigramOneHotMLP()\n",
        "\n",
        "# 1f) Training loop\n",
        "optimizer = torch.optim.Adam(bigram_one_hot_mlp.parameters(), lr=0.01)\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    logits = bigram_one_hot_mlp(inputs_one_hot.float())\n",
        "    loss = F.cross_entropy(logits, outputs_one_hot.float())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(bigram_one_hot_mlp.generate())"
      ],
      "metadata": {
        "id": "hoMGZgEOdRjt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbd3e3e5-2862-4495-b730-fd5b5088674d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "arsooolithe ocolie the l:\n",
            "st brs thispre ow Maveve t\n",
            "\n",
            "\n",
            "s Marth\n",
            "\n",
            "Alld.\n",
            "\n",
            "\n",
            "Onthary lll eled Cit, paf ars\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1g) Create embedding inputs and outputs\n",
        "def create_embedding_inputs_and_outputs() -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    input_ids = torch.tensor([chars.index(c) for c in text[:-1]])\n",
        "    output_ids = torch.tensor([chars.index(c) for c in text[1:]])\n",
        "    return input_ids, F.one_hot(output_ids, num_classes=len(chars)).float()\n",
        "\n",
        "input_ids, outputs_one_hot = create_embedding_inputs_and_outputs()\n",
        "\n",
        "# 1h) Implement BigramEmbeddingMLP\n",
        "class BigramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(len(chars), 8)\n",
        "        self.fc1 = nn.Linear(8, 8)\n",
        "        self.fc2 = nn.Linear(8, len(chars))\n",
        "        self.activation = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def generate(self, start='a', max_new_tokens=100) -> str:\n",
        "        out = start\n",
        "        for _ in range(max_new_tokens):\n",
        "            x = torch.tensor([chars.index(out[-1])])\n",
        "            logits = self(x)\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_char = chars[torch.multinomial(probs, 1).item()]\n",
        "            out += next_char\n",
        "        return out\n",
        "\n",
        "bigram_embedding_mlp = BigramEmbeddingMLP()\n",
        "\n",
        "# Training loop\n",
        "optimizer = torch.optim.Adam(bigram_embedding_mlp.parameters(), lr=0.01)\n",
        "for _ in range(1000):\n",
        "    optimizer.zero_grad()\n",
        "    logits = bigram_embedding_mlp(input_ids)\n",
        "    loss = F.cross_entropy(logits, outputs_one_hot)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(bigram_embedding_mlp.generate())"
      ],
      "metadata": {
        "id": "PasrfDz-dSqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ebe12e-cf64-478a-9558-bd1fa2862d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ald.\n",
            "ol: ues forecisun:\n",
            "Fit pes wend n\n",
            "\n",
            "Len: es t atir humizele Cis partiss't us ge is:\n",
            "beroiraoforn:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Generative Pretrained Transformer (65 points)\n",
        "\n",
        "For this part, it is best to use a gpu. In the settings at the top go to Runtime -> Change Runtime Type and select T4 GPU"
      ],
      "metadata": {
        "id": "qplpM8_Cbp0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# run nvidia-smi to check gpu usage\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "0Oh-3FeFxxnI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f386b3-4639-4e66-8784-ed5e3139e740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Oct 14 02:57:22 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8              11W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the gpt model, let's use the full text\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "rhJAwCAOADP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a character level tokenization function.\n",
        "\n",
        "1. Create a list of unique characters in the string. (1 points)\n",
        "2. Implement a function `encode(s: str) -> list[int]` that takes a string and returns a list of ids (1 point)\n",
        "3. Implement a function `decode(ids: list[int]) -> str` that takes a list of ids (ints) and returns a string (1 point)\n"
      ],
      "metadata": {
        "id": "z_LZpvZ8AEEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of unique characters in the text (character set)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Mapping from character to integer ID and reverse\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }  # String to ID (char -> id)\n",
        "itos = { i:ch for i,ch in enumerate(chars) }  # ID to String (id -> char)\n",
        "\n",
        "def encode(s: str) -> list[int]:\n",
        "    return [stoi[c] for c in s]\n",
        "\n",
        "def decode(ids: list[int]) -> str:\n",
        "    return ''.join([itos[i] for i in ids])\n",
        "\n",
        "# Checking vocab size and mappings\n",
        "print(f'Vocab Size: {vocab_size}')\n",
        "print(f'Sample chars to ids: {[(ch, stoi[ch]) for ch in chars[:10]]}')  # Sample of chars and their ids\n",
        "print(f'Sample ids to chars: {[(i, itos[i]) for i in range(10)]}')  # Sample of ids and their chars"
      ],
      "metadata": {
        "id": "rnEOfMj4Dk4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de986fb9-9c16-45cc-89d3-2212c7c83d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab Size: 65\n",
            "Sample chars to ids: [('\\n', 0), (' ', 1), ('!', 2), ('$', 3), ('&', 4), (\"'\", 5), (',', 6), ('-', 7), ('.', 8), ('3', 9)]\n",
            "Sample ids to chars: [(0, '\\n'), (1, ' '), (2, '!'), (3, '$'), (4, '&'), (5, \"'\"), (6, ','), (7, '-'), (8, '.'), (9, '3')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.long).cuda()"
      ],
      "metadata": {
        "id": "1gyOaRF5Dq1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 16\n",
        "data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvWGi8Mk6x1q",
        "outputId": "87af04e8-595f-42c8-de2f-6527e01d7f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train a transformer, we feed the model `n` tokens (context) and try to predict the `n+1`th token (target) in the sequence.\n",
        "\n"
      ],
      "metadata": {
        "id": "GvO4hSK171Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = data[:block_size]\n",
        "y = data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVWxO6Pa70Lh",
        "outputId": "585a914c-d2ce-4a98-affa-4d63aa9fe5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18], device='cuda:0') the target: 47\n",
            "when input is tensor([18, 47], device='cuda:0') the target: 56\n",
            "when input is tensor([18, 47, 56], device='cuda:0') the target: 57\n",
            "when input is tensor([18, 47, 56, 57], device='cuda:0') the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58], device='cuda:0') the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1], device='cuda:0') the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15], device='cuda:0') the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47], device='cuda:0') the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58], device='cuda:0') the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47], device='cuda:0') the target: 64\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64], device='cuda:0') the target: 43\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43], device='cuda:0') the target: 52\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52], device='cuda:0') the target: 10\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10],\n",
            "       device='cuda:0') the target: 0\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0],\n",
            "       device='cuda:0') the target: 14\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14],\n",
            "       device='cuda:0') the target: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def get_batch():\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "print(get_batch())"
      ],
      "metadata": {
        "id": "lFYZnm2MuLlt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "576bef0b-7907-4aef-9346-15ce3ec238ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 1, 58, 46,  ..., 54, 53, 61],\n",
            "        [45,  1, 63,  ..., 57, 47, 56],\n",
            "        [ 1, 39, 52,  ..., 47, 52,  1],\n",
            "        ...,\n",
            "        [56, 39, 52,  ..., 58,  1, 51],\n",
            "        [ 1, 21, 34,  ..., 43,  1, 54],\n",
            "        [53, 44,  1,  ..., 42, 57,  8]], device='cuda:0'), tensor([[58, 46, 43,  ..., 53, 61, 43],\n",
            "        [ 1, 63, 53,  ..., 47, 56, 43],\n",
            "        [39, 52, 42,  ..., 52,  1, 46],\n",
            "        ...,\n",
            "        [39, 52, 45,  ...,  1, 51, 53],\n",
            "        [21, 34, 10,  ...,  1, 54, 39],\n",
            "        [44,  1, 58,  ..., 57,  8,  1]], device='cuda:0'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single Self Attention Head (5 points)\n",
        "![](https://i.ibb.co/GWR1XG0/head.png)"
      ],
      "metadata": {
        "id": "-HmnXJjxtm3N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    def __init__(self, head_size, embed_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size\n",
        "\n",
        "        # Linear transformations for keys, queries, and values (projecting to head_size)\n",
        "        self.key = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.query = nn.Linear(embed_size, head_size, bias=False)\n",
        "        self.value = nn.Linear(embed_size, head_size, bias=False)\n",
        "\n",
        "        self.proj = nn.Linear(head_size, head_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # B: batch size, T: sequence length, C: embedding size\n",
        "\n",
        "        # Calculate keys, queries, and values\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Compute attention scores (scaled dot-product)\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
        "\n",
        "        # Apply softmax to get attention weights\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        attention_output = attn_weights @ v\n",
        "\n",
        "        # Apply output projection\n",
        "        out = self.proj(attention_output)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Testing the SelfAttentionHead\n",
        "embed_size = 64\n",
        "head_size = 16\n",
        "x = torch.randn(8, 32, embed_size).cuda()\n",
        "\n",
        "# Initialize and test\n",
        "attention_head = SelfAttentionHead(head_size, embed_size).cuda()\n",
        "output = attention_head(x)\n",
        "\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "5SD8Z16R-sfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd41db83-0900-4ccc-bb64-e142ca543729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 32, 64])\n",
            "Output shape: torch.Size([8, 32, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Self Attention (5 points)\n",
        "\n",
        "`constructor`\n",
        "\n",
        "- Create 4 `SelfAttentionHead` instances. Consider using `nn.ModuleList`\n",
        "- Create a linear layer with n_embd input dim and n_embd output dim\n",
        "\n",
        "`forward`\n",
        "\n",
        "In the forward implementation, pass `x` through each head, then concatenate all the outputs along the feature dimension, then pass the concatenated output through the linear layer\n",
        "\n",
        "![](https://i.ibb.co/y5SwyZZ/multihead.png)"
      ],
      "metadata": {
        "id": "LWeoHGBiFpWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size, num_embed, dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the attention heads\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(head_size, num_embed) for _ in range(num_heads)])\n",
        "\n",
        "        # Linear layer to project the concatenated heads' output\n",
        "        self.proj = nn.Linear(head_size * num_heads, num_embed)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Concatenate the outputs of all heads along the feature dimension\n",
        "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "\n",
        "        # Apply linear projection and dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "\n",
        "        return out\n",
        "\n",
        "# Testing MultiHeadAttention\n",
        "num_heads = 4\n",
        "head_size = 16\n",
        "num_embed = 64\n",
        "\n",
        "x = torch.randn(8, 32, num_embed).cuda()\n",
        "\n",
        "# Initialize and test\n",
        "multi_head_attention = MultiHeadAttention(num_heads, head_size, num_embed).cuda()\n",
        "output = multi_head_attention(x)\n",
        "\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "gFsPDkpnFs_b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19ff5044-53a8-49b2-d878-732b4dde9f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 32, 64])\n",
            "Output shape: torch.Size([8, 32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP (2 points)\n",
        "Implement a 2 layer MLP\n",
        "\n",
        "\n",
        "![](https://i.ibb.co/C0DtrF5/ff.png)"
      ],
      "metadata": {
        "id": "uH_0ELyZ8YCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size=256, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # First linear layer to project up to hidden size\n",
        "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "\n",
        "        # Second linear layer to project back to embed size\n",
        "        self.fc2 = nn.Linear(hidden_size, embed_size)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # First layer, followed by ReLU\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Second layer and dropout\n",
        "        x = self.dropout(self.fc2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "# Testing MLP\n",
        "embed_size = 64  # Example embedding size\n",
        "x = torch.randn(8, 32, embed_size).cuda()\n",
        "\n",
        "# Initialize and test\n",
        "mlp = MLP(embed_size).cuda()\n",
        "output = mlp(x)\n",
        "\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "K96Z3kAv7lNt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788f6268-8a3c-4081-b1dc-7b1706ee8e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 32, 64])\n",
            "Output shape: torch.Size([8, 32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer block (20 points)\n",
        "\n",
        "Layer normalization help training stability by normalizing the outputs of neurons within a single layer across all features for each individual data point, not across a full batch or a specific feature.\n",
        "\n",
        "Dropout is a form of regularization to prevent overfitting.\n",
        "\n",
        "This is the diagram of a transformer block:\n",
        "\n",
        "![](https://i.ibb.co/X85C473/block.png)"
      ],
      "metadata": {
        "id": "bUFxuyf-JIxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(n_embd, n_head, dropout=dropout)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(n_embd * 4, n_embd),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_attn, _ = self.attn(x, x, x)\n",
        "        x = self.ln1(x + x_attn)\n",
        "        x_ffn = self.ffn(x)\n",
        "        x = self.ln2(x + x_ffn)\n",
        "        return x\n",
        "\n",
        "# Testing the Block\n",
        "n_embd = 64  # Dimensionality of the embeddings\n",
        "n_head = 4   # Number of attention heads\n",
        "x = torch.randn(8, 32, n_embd).cuda()  # Example input: batch_size=8, seq_len=32, n_embd=64\n",
        "\n",
        "# Initialize and test the Block\n",
        "transformer_block = Block(n_embd, n_head).cuda()\n",
        "output = transformer_block(x)\n",
        "\n",
        "print(f'Input shape: {x.shape}')\n",
        "print(f'Output shape: {output.shape}')"
      ],
      "metadata": {
        "id": "xTDAd66KIvvx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64eeede1-78c2-49e4-f038-3d712b825e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 32, 64])\n",
            "Output shape: torch.Size([8, 32, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT\n",
        "\n",
        "`constructor` (5 points)\n",
        "\n",
        "1. create the token embedding table and the position embedding table\n",
        "2. create variable `self.blocks` that is a series of 4 `Block`s. The data will pass through each block sequentially. Consider using `nn.Sequential`\n",
        "3. create a layer norm layer\n",
        "4. create a linear layer for predicting the next token\n",
        "\n",
        "`forward(self, idx, targets=None)`. (5 points)\n",
        "\n",
        "`forward` takes a batch of context ids as input of size (B, T) and returns the logits and the loss, if targets is not None. If targets is None, return the logits and None.\n",
        "1. get the token by using the token embedding table created in the constructor\n",
        "2. create the position embeddings\n",
        "3. sum the token and position embeddings to get the model input\n",
        "4. pass the model through the blocks, the layernorm layer, and the final linear layer\n",
        "5. compute the loss\n",
        "\n",
        "`generate(start_char, max_new_tokens, top_p, top_k, temperature) -> str` (5 points)\n",
        "1. implement top p, top_k, and temperature for sampling\n",
        "\n"
      ],
      "metadata": {
        "id": "SyFQXltDKNti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://i.ibb.co/n8sbQ0V/Screenshot-2024-01-23-at-8-59-08-PM.png)"
      ],
      "metadata": {
        "id": "0Xa2bh2XDdKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, block_size, num_blocks, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # Stacking transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head, dropout) for _ in range(num_blocks)])\n",
        "\n",
        "        self.layer_norm = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "\n",
        "        # Token and position embeddings\n",
        "        token_emb = self.token_embedding(idx)\n",
        "        position_ids = torch.arange(T, device=idx.device).unsqueeze(0)\n",
        "        position_emb = self.position_embedding(position_ids)\n",
        "\n",
        "        # Combined embeddings\n",
        "        x = token_emb + position_emb\n",
        "\n",
        "        # Pass through transformer blocks\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # Layer normalization\n",
        "        x = self.layer_norm(x)\n",
        "\n",
        "        # Final linear layer\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # Compute loss if targets are provided\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "\n",
        "            shift_logits = logits[:, :-1, :].contiguous()\n",
        "            shift_targets = targets[:, 1:].contiguous()\n",
        "            loss = F.cross_entropy(shift_logits.view(-1, logits.size(-1)), shift_targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, start_char, max_new_tokens, top_p, top_k, temperature):\n",
        "        self.eval()\n",
        "        generated = [start_char]\n",
        "        input_ids = torch.tensor(generated, device='cuda').unsqueeze(0)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Forward pass\n",
        "            with torch.no_grad():\n",
        "                logits, _ = self.forward(input_ids)\n",
        "                logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Top-k and top-p filtering\n",
        "            if top_k is not None:\n",
        "                top_k_values, top_k_indices = torch.topk(logits, top_k)\n",
        "                logits[logits < top_k_values[:, -1, None]] = float('-inf')\n",
        "\n",
        "            if top_p is not None:\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "                sorted_indices_to_remove[:, 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = float('-inf')\n",
        "\n",
        "            # Sample from the filtered distribution\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append new token while ensuring dimensions are compatible\n",
        "            input_ids = torch.cat([input_ids, next_token.t()], dim=1)\n",
        "            generated.append(next_token.item())\n",
        "\n",
        "        return generated\n",
        "\n",
        "# Testing the GPT model\n",
        "vocab_size = 100\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "block_size = 32\n",
        "num_blocks = 4\n",
        "\n",
        "# Create an instance of the GPT model\n",
        "gpt_model = GPT(vocab_size, n_embd, n_head, block_size, num_blocks).cuda()\n",
        "\n",
        "# Example input for testing\n",
        "idx = torch.randint(0, vocab_size, (8, block_size)).cuda()\n",
        "logits, loss = gpt_model(idx, targets=None)\n",
        "\n",
        "print(f'Input shape: {idx.shape}')\n",
        "print(f'Output shape (logits): {logits.shape}')\n",
        "print(f'Loss: {loss}')\n",
        "\n",
        "# Testing the generate function\n",
        "start_char = idx[0, 0].item()\n",
        "generated_text = gpt_model.generate(start_char, max_new_tokens=10, top_p=0.9, top_k=50, temperature=1.0)\n",
        "\n",
        "print(f'Generated text tokens: {generated_text}')"
      ],
      "metadata": {
        "id": "8WT4oUN084ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944c6ddb-c7ac-48e5-9c47-df7292d0a890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([8, 32])\n",
            "Output shape (logits): torch.Size([8, 32, 100])\n",
            "Loss: None\n",
            "Generated text tokens: [22, 77, 56, 28, 32, 96, 1, 86, 37, 76, 65]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training loop (15 points)\n",
        "\n",
        "implement training loop"
      ],
      "metadata": {
        "id": "Njzrwwiv-mfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "vocab_size = 100\n",
        "vocab = [f\"token_{i}\" for i in range(vocab_size)]  # Simple token vocabulary\n",
        "vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_vocab = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Model configuration\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "block_size = 32\n",
        "num_blocks = 4\n",
        "\n",
        "model = GPT(vocab_size, n_embd, n_head, block_size, num_blocks).to('cuda')\n",
        "\n",
        "# Define a loss function and optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "# Hyperparameters\n",
        "max_iters = 5000\n",
        "batch_size = 8\n",
        "\n",
        "# Generate random input and target data for training\n",
        "data = torch.randint(0, vocab_size, (max_iters, batch_size, block_size)).to('cuda')\n",
        "targets = torch.randint(0, vocab_size, (max_iters, batch_size, block_size)).to('cuda')\n",
        "\n",
        "# Training loop\n",
        "for iter in range(max_iters):\n",
        "    # Get a batch of data\n",
        "    idx = data[iter]\n",
        "    target = targets[iter]\n",
        "\n",
        "    # Zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass\n",
        "    logits, loss = model(idx, targets=target)\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print the loss every 100 iterations\n",
        "    if iter % 100 == 0:\n",
        "        print(f\"Iteration {iter}, Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "qWtn2uTwYUrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ba91cc8-eb73-4fd2-ddbd-2d815513c51b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Loss: 4.7231\n",
            "Iteration 100, Loss: 4.6127\n",
            "Iteration 200, Loss: 4.6170\n",
            "Iteration 300, Loss: 4.5989\n",
            "Iteration 400, Loss: 4.6244\n",
            "Iteration 500, Loss: 4.6057\n",
            "Iteration 600, Loss: 4.6040\n",
            "Iteration 700, Loss: 4.6046\n",
            "Iteration 800, Loss: 4.6020\n",
            "Iteration 900, Loss: 4.6156\n",
            "Iteration 1000, Loss: 4.6086\n",
            "Iteration 1100, Loss: 4.6108\n",
            "Iteration 1200, Loss: 4.6173\n",
            "Iteration 1300, Loss: 4.6077\n",
            "Iteration 1400, Loss: 4.6068\n",
            "Iteration 1500, Loss: 4.6083\n",
            "Iteration 1600, Loss: 4.6042\n",
            "Iteration 1700, Loss: 4.6140\n",
            "Iteration 1800, Loss: 4.6026\n",
            "Iteration 1900, Loss: 4.6041\n",
            "Iteration 2000, Loss: 4.6128\n",
            "Iteration 2100, Loss: 4.6117\n",
            "Iteration 2200, Loss: 4.6151\n",
            "Iteration 2300, Loss: 4.6121\n",
            "Iteration 2400, Loss: 4.6099\n",
            "Iteration 2500, Loss: 4.6125\n",
            "Iteration 2600, Loss: 4.6111\n",
            "Iteration 2700, Loss: 4.6122\n",
            "Iteration 2800, Loss: 4.6008\n",
            "Iteration 2900, Loss: 4.6191\n",
            "Iteration 3000, Loss: 4.6064\n",
            "Iteration 3100, Loss: 4.6033\n",
            "Iteration 3200, Loss: 4.6058\n",
            "Iteration 3300, Loss: 4.6060\n",
            "Iteration 3400, Loss: 4.6119\n",
            "Iteration 3500, Loss: 4.6097\n",
            "Iteration 3600, Loss: 4.6083\n",
            "Iteration 3700, Loss: 4.6118\n",
            "Iteration 3800, Loss: 4.6099\n",
            "Iteration 3900, Loss: 4.6056\n",
            "Iteration 4000, Loss: 4.6109\n",
            "Iteration 4100, Loss: 4.6068\n",
            "Iteration 4200, Loss: 4.6086\n",
            "Iteration 4300, Loss: 4.6071\n",
            "Iteration 4400, Loss: 4.6110\n",
            "Iteration 4500, Loss: 4.6067\n",
            "Iteration 4600, Loss: 4.6123\n",
            "Iteration 4700, Loss: 4.6014\n",
            "Iteration 4800, Loss: 4.6114\n",
            "Iteration 4900, Loss: 4.6144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text\n",
        "\n",
        "\n",
        "print some text that your model generates"
      ],
      "metadata": {
        "id": "zy3v8Nv7YVUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_token, gen_length=50, temperature=1.0):\n",
        "    model.eval()\n",
        "    generated = [vocab_to_idx[start_token]]\n",
        "\n",
        "    # Create the initial input tensor\n",
        "    input_tensor = torch.tensor(generated).unsqueeze(0).to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(gen_length):\n",
        "            logits, _ = model(input_tensor)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probabilities = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "            # Append the generated token to the list\n",
        "            generated.append(next_token.item())\n",
        "\n",
        "            # Prepare the next input tensor\n",
        "            input_tensor = torch.cat((input_tensor, next_token.view(1, 1)), dim=1)\n",
        "\n",
        "            # Check if input_tensor exceeds block size\n",
        "            if input_tensor.size(1) > block_size:\n",
        "                input_tensor = input_tensor[:, -block_size:]\n",
        "\n",
        "    # Convert token indices to text using the vocabulary mapping\n",
        "    generated_text = [idx_to_vocab[token] for token in generated]\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text starting with a specific token from the vocabulary\n",
        "start_token = \"token_0\"\n",
        "generated_sequence = generate_text(model, start_token, gen_length=50, temperature=0.8)\n",
        "\n",
        "# Print the generated text\n",
        "print(\"Generated Text:\", ' '.join(generated_sequence))"
      ],
      "metadata": {
        "id": "wO80o7u-zfb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558f381e-f9dd-4973-9753-aece5bf4e4ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: token_0 token_13 token_15 token_1 token_47 token_74 token_73 token_70 token_1 token_73 token_51 token_91 token_49 token_83 token_77 token_99 token_0 token_32 token_31 token_88 token_74 token_55 token_4 token_59 token_5 token_60 token_1 token_63 token_53 token_33 token_74 token_25 token_13 token_37 token_70 token_97 token_24 token_15 token_42 token_9 token_60 token_97 token_95 token_57 token_40 token_85 token_3 token_96 token_33 token_63 token_25\n"
          ]
        }
      ]
    }
  ]
}